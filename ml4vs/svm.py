# -*- coding: utf-8 -*-
import os
import numpy as np
from sklearn import decomposition
from sklearn.cross_validation import StratifiedKFold
from sklearn.model_selection import cross_val_predict, cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score
from sklearn.svm import SVC, LinearSVC
import hyperopt
from hyperopt import hp, fmin, tpe, STATUS_OK, Trials
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import Imputer
from sklearn.preprocessing import StandardScaler
from data_load import load_data, load_data_tgt


data_dir = '/home/ilya/code/ml4vs/data/LMC_SC20__corrected_list_of_variables/raw_index_values'
file_1 = 'vast_lightcurve_statistics_variables_only.log'
file_0 = 'vast_lightcurve_statistics_constant_only.log'
file_0 = os.path.join(data_dir, file_0)
file_1 = os.path.join(data_dir, file_1)
names = ['Magnitude', 'clipped_sigma', 'meaningless_1', 'meaningless_2',
         'star_ID', 'weighted_sigma', 'skew', 'kurt', 'I', 'J', 'K', 'L',
         'Npts', 'MAD', 'lag1', 'RoMS', 'rCh2', 'Isgn', 'Vp2p', 'Jclp', 'Lclp',
         'Jtim', 'Ltim', 'CSSD', 'Ex', 'inv_eta', 'E_A', 'S_B', 'NXS', 'IQR']
# names_to_delete = ['meaningless_1', 'meaningless_2', 'star_ID',
#                    'Npts', 'CSSD']
names_to_delete = ['meaningless_1', 'meaningless_2', 'star_ID',
                   'Npts', 'CSSD', 'clipped_sigma', 'lag1', 'L', 'Lclp', 'Jclp',
                   'MAD', 'Ltim']

X, y, df, features_names, delta = load_data([file_0, file_1], names,
                                            names_to_delete)
from sklearn.cluster import DBSCAN
# X = StandardScaler().fit_transform(X)
# db = DBSCAN(eps=0.35, min_samples=18).fit(X)
# labels = db.labels_
# print(len(set(labels)))
# X = np.hstack((X, labels[:, np.newaxis]))
target = 'variable'
predictors = list(df)
predictors.remove(target)
dtrain = df

kfold = StratifiedKFold(dtrain[target], n_folds=4, shuffle=True, random_state=1)


# def objective(space):
#     print space
#     # if space['kernel'] == 'rbf':
#     clf = SVC(C=space['C'], class_weight={0: 1, 1: space['cw']},
#               probability=False, gamma=space['gamma'], random_state=1)
#     # elif space['kernel'] == 'linear':
#     # clf = SVC(C=space['C'], kernel='linear',
#     #           class_weight={0: 1, 1: space['cw']},
#     #           probability=False, random_state=1)
#     # elif space['kernel'] == 'poly':
#     # clf = SVC(C=space['C'], class_weight={0: 1, 1: space['cw']},
#     #           probability=False, random_state=1, kernel='poly',
#     #           degree=2, gamma=space['gamma'])
#     # clf = LinearSVC(C=space['C'], class_weight={0: 1, 1: space['cw']},
#     #                 penalty='l1', dual=False)
#
#
#     # pca = decomposition.PCA(n_components=space['n_pca'], random_state=1)
#     estimators = list()
#     estimators.append(('imputer', Imputer(missing_values='NaN', strategy='median',
#                                           axis=0, verbose=2)))
#     estimators.append(('scaler', StandardScaler()))
#     # estimators.append(('pca', pca))
#     estimators.append(('clf', clf))
#     pipeline = Pipeline(estimators)
#
#     # auc = np.mean(cross_val_score(pipeline, X, y, cv=kfold, scoring='roc_auc',
#     #                               verbose=1, n_jobs=4))
#     y_preds = cross_val_predict(pipeline, X, y, cv=kfold, n_jobs=4)
#     CMs = list()
#     for train_idx, test_idx in kfold:
#         CMs.append(confusion_matrix(y[test_idx], y_preds[test_idx]))
#     CM = np.sum(CMs, axis=0)
#
#     FN = CM[1][0]
#     TP = CM[1][1]
#     FP = CM[0][1]
#     print "TP = {}".format(TP)
#     print "FP = {}".format(FP)
#     print "FN = {}".format(FN)
#
#     f1 = 2. * TP / (2. * TP + FP + FN)
#     print "F1 : ", f1
#
#     return{'loss': 1-f1, 'status': STATUS_OK}
#
# space = {'C': hp.loguniform('C', 2.3, 3.6),
#          'gamma': hp.loguniform('gamma', -6.2, -1.6),
#          'cw': hp.uniform('cw', 1, 6)}
#

# trials = Trials()
# best = fmin(fn=objective,
#             space=space,
#             algo=tpe.suggest,
#             max_evals=200,
#             trials=trials)

# print hyperopt.space_eval(space, best)
# best_pars = hyperopt.space_eval(space, best)

clf = SVC(C=25.053, gamma=0.0173, class_weight={0: 1, 1: 2.93},
          probability=True)
estimators = list()
estimators.append(('imputer', Imputer(missing_values='NaN', strategy='median',
                                      axis=0, verbose=2)))
estimators.append(('scaler', StandardScaler()))
estimators.append(('clf', clf))
pipeline = Pipeline(estimators)
pipeline.fit(X, y)

# Load blind test data
file_tgt = 'LMC_SC19_PSF_Pgood98__vast_lightcurve_statistics.log'
file_tgt = os.path.join(data_dir, file_tgt)
X_tgt, feature_names, df, df_orig = load_data_tgt(file_tgt, names, names_to_delete,
                                                  delta)
# Predict clases on new data
y_probs = pipeline.predict_proba(X_tgt)[:, 1]
idx = y_probs > 0.5
idx_ = y_probs < 0.5
gb_no = list(df_orig['star_ID'][idx_])
print("Found {} variables".format(np.count_nonzero(idx)))

with open('svm_new_results.txt', 'w') as fo:
    for line in list(df_orig['star_ID'][idx]):
        fo.write(line + '\n')

# Check F1
with open('clean_list_of_new_variables.txt', 'r') as fo:
    news = fo.readlines()
news = [line.strip().split(' ')[1] for line in news]
news = set(news)

with open('svm_new_results.txt', 'r') as fo:
    gb = fo.readlines()
gb = [line.strip().split('_')[4].split('.')[0] for line in gb]
gb = set(gb)

print "Among new vars found {}".format(len(news.intersection(gb)))

with open('candidates_50perc_threshold.txt', 'r') as fo:
    c50 = fo.readlines()
c50 = [line.strip("\", ', \", \n, }, {") for line in c50]

with open('variables_not_in_catalogs.txt', 'r') as fo:
    not_in_cat = fo.readlines()
nic = [line.strip().split(' ')[1] for line in not_in_cat]

# Catalogue variables
cat_vars = set(c50).difference(set(nic))
# Non-catalogue variable
noncat_vars = set([line.strip().split(' ')[1] for line in not_in_cat if 'CST'
                   not in line])

# All variables
all_vars = news.union(cat_vars).union(noncat_vars)
gb_no = set([line.strip().split('_')[4].split('.')[0] for line in gb_no])

found_bad = '181193' in gb
print "Found known variable : ", found_bad

FN = len(gb_no.intersection(all_vars))
TP = len(all_vars.intersection(gb))
TN = len(gb_no) - FN
FP = len(gb) - TP
recall = float(TP) / (TP + FN)
precision = float(TP) / (TP + FP)
F1 = 2 * precision * recall / (precision + recall)
print "precision: {}".format(precision)
print "recall: {}".format(recall)
print "F1: {}".format(F1)
print "TN={}, FP={}".format(TN, FP)
print "FN={}, TP={}".format(FN, TP)

